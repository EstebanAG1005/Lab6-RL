## Task 2

### An√°lisis de los resultados de MCTS y Dyna-Q+

#### a. Comparaci√≥n de resultados de MCTS y Dyna-Q+

1. **Tasa de √©xito**:
   - **MCTS**: La tasa de √©xito para MCTS comienza a mejorar r√°pidamente en los primeros episodios y se estabiliza alrededor del 0.75 despu√©s de unas 20,000 iteraciones. El comportamiento parece ser consistente con una tasa de √©xito que se acerca a un valor m√°ximo cercano a 0.75 a lo largo del tiempo.
   - **Dyna-Q+**: El comportamiento de la tasa de √©xito de Dyna-Q+ es bastante similar a MCTS, con una r√°pida convergencia inicial, alcanzando un valor estable cercano a 0.78 despu√©s de 20,000 episodios y manteni√©ndose constante a partir de ah√≠.

2. **N√∫mero de pasos hasta el √©xito (Convergencia)**:
   - **MCTS**: La cantidad de pasos para alcanzar el √©xito se estabiliza en un rango de 40 a 50 pasos por episodio una vez que el agente ha aprendido la tarea. Esto muestra que MCTS logra encontrar una estrategia bastante consistente que toma entre 40 y 50 pasos para completar la tarea de manera repetida.
   - **Dyna-Q+**: En contraste, Dyna-Q+ tiene una variabilidad mucho mayor en el n√∫mero de pasos para alcanzar el √©xito, con algunos episodios requiriendo tan solo unos pocos pasos (alrededor de 50), mientras que otros requieren m√°s de 200 pasos. Esto indica que el comportamiento de Dyna-Q+ es menos consistente en t√©rminos de eficiencia por episodio.

#### b. Fortalezas y debilidades de cada enfoque en el contexto de FrozenLake-v1

1. **MCTS**:
   - **Fortalezas**: MCTS parece ser bastante robusto en t√©rminos de consistencia. Una vez que ha convergido, el n√∫mero de pasos por episodio se mantiene estable en un rango relativamente estrecho (40-50 pasos), lo que sugiere que sigue una pol√≠tica bien definida. Su tasa de √©xito tambi√©n se estabiliza en un valor relativamente alto (~0.75).
   - **Debilidades**: A pesar de la consistencia, MCTS puede tener dificultades en entornos extremadamente estoc√°sticos como FrozenLake, ya que la toma de decisiones basada en la b√∫squeda de √°rboles puede no capturar completamente la variabilidad del entorno. Adem√°s, el proceso de b√∫squeda puede ser computacionalmente costoso en comparaci√≥n con otros m√©todos.

2. **Dyna-Q+**:
   - **Fortalezas**: Dyna-Q+ tiene la ventaja de incorporar actualizaciones de modelos y planificaciones en su aprendizaje, lo que le permite aprender de manera eficiente y tener un comportamiento que converge r√°pidamente. Adem√°s, su tasa de √©xito es ligeramente superior a la de MCTS, lo que indica que, en promedio, es capaz de adaptarse mejor a las incertidumbres del entorno.
   - **Debilidades**: La mayor debilidad de Dyna-Q+ es la variabilidad en el n√∫mero de pasos para completar un episodio. Aunque su tasa de √©xito es alta, la eficiencia en t√©rminos de pasos es mucho menos estable, lo que podr√≠a causar problemas en situaciones donde la eficiencia en el n√∫mero de pasos es cr√≠tica.

#### c. Impacto de la naturaleza estoc√°stica del entorno en el rendimiento de ambos algoritmos

La naturaleza estoc√°stica de FrozenLake introduce incertidumbre en la transici√≥n entre los estados, lo que afecta el rendimiento de ambos algoritmos de diferentes maneras:

- **MCTS**: En entornos estoc√°sticos, la b√∫squeda de √°rboles puede volverse ineficiente si el √°rbol generado durante la simulaci√≥n no es representativo de las transiciones reales debido a la variabilidad del entorno. Esto puede explicar por qu√©, aunque MCTS es consistente en el n√∫mero de pasos, su tasa de √©xito no alcanza valores tan altos como podr√≠a esperarse en un entorno determinista.

- **Dyna-Q+**: La planificaci√≥n en Dyna-Q+ le permite aprender sobre transiciones estoc√°sticas al realizar actualizaciones basadas en experiencias reales y simuladas. Sin embargo, la estocasticidad tambi√©n introduce incertidumbre en el n√∫mero de pasos para completar una tarea, lo que podr√≠a explicar la alta variabilidad observada en la cantidad de pasos por episodio, incluso despu√©s de muchas iteraciones.

![Average Reward] (./img/average_reward_per_episode)


1. Estrategias de exploraci√≥n:

a. ¬øC√≥mo influye la bonificaci√≥n de exploraci√≥n en Dyna-Q+ en la pol√≠tica en comparaci√≥n con el equilibrio de exploraci√≥n-explotaci√≥n en MCTS? ¬øQu√© enfoque conduce a una convergencia m√°s r√°pida en el entorno FrozenLake-v1?

La bonificaci√≥n de exploraci√≥n en Dyna-Q+ influye en la pol√≠tica al promover una mayor exploraci√≥n de estados desconocidos o menos visitados. Esto puede llevar a una convergencia m√°s r√°pida en el entorno FrozenLake-v1 en comparaci√≥n con el equilibrio de exploraci√≥n-explotaci√≥n en MCTS. MCTS tiende a concentrarse m√°s en la explotaci√≥n de las acciones que han demostrado ser m√°s prometedoras, lo que puede ralentizar la exploraci√≥n de todo el espacio de estados. La bonificaci√≥n de exploraci√≥n en Dyna-Q+ ayuda a que el agente explore m√°s activamente el entorno, lo que puede resultar en una convergencia m√°s r√°pida a la pol√≠tica √≥ptima.

2. Rendimiento del algoritmo:

a. ¬øQu√© algoritmo, MCTS o Dyna-Q+, tuvo un mejor rendimiento en t√©rminos de tasa de √©xito y recompensa promedio en el entorno FrozenLake-v1? Analice por qu√© uno podr√≠a superar al otro dada la naturaleza estoc√°stica del entorno.

En general, el rendimiento de Dyna-Q+ suele ser mejor que MCTS en el entorno FrozenLake-v1, con una mayor tasa de √©xito y recompensa promedio. Esto se debe a que Dyna-Q+ puede aprovechar mejor la naturaleza estoc√°stica del entorno. Al actualizar el modelo interno y planificar utilizando este modelo, Dyna-Q+ puede anticipar y adaptarse mejor a las transiciones probabil√≠sticas. Por otro lado, MCTS se basa m√°s en la exploraci√≥n aleatoria del √°rbol de b√∫squeda, lo que puede ser menos eficiente en entornos con alta aleatoriedad.

3. Impacto de las transiciones estoc√°sticas:

a. ¬øC√≥mo afectan las transiciones probabil√≠sticas en FrozenLake-v1 al proceso de planificaci√≥n en MCTS en comparaci√≥n con Dyna-Q+? ¬øQu√© algoritmo es m√°s robusto a la aleatoriedad introducid por el entorno?

Las transiciones probabil√≠sticas en FrozenLake-v1 afectan m√°s al proceso de planificaci√≥n en MCTS que a Dyna-Q+. MCTS debe explorar el √°rbol de b√∫squeda de forma m√°s exhaustiva para tener en cuenta la aleatoriedad, lo que puede ralentizar la convergencia. En cambio, Dyna-Q+ puede aprovechar su modelo interno para anticipar y adaptarse mejor a las transiciones estoc√°sticas. Esto hace que Dyna-Q+ sea m√°s robusto a la aleatoriedad introducida por el entorno.

4. Sensibilidad de los par√°metros:

a. En la implementaci√≥n de Dyna-Q+, ¬øc√≥mo afecta el cambio de la cantidad de pasos de planificaci√≥n ùëõ y la bonificaci√≥n de exploraci√≥n a la curva de aprendizaje y al rendimiento final? ¬øSe necesitar√≠an diferentes configuraciones para una versi√≥n determinista del entorno?

En la implementaci√≥n de Dyna-Q+, el aumento del n√∫mero de pasos de planificaci√≥n 'n' y de la bonificaci√≥n de exploraci√≥n generalmente mejorar√° la curva de aprendizaje y el rendimiento final. Un mayor 'n' permite al agente planificar m√°s a fondo utilizando su modelo interno, mientras que una mayor bonificaci√≥n de exploraci√≥n lo incentiva a explorar estados menos conocidos. Sin embargo, existe un equilibrio, ya que valores excesivamente altos de estos par√°metros pueden llevar a una exploraci√≥n demasiado agresiva y afectar negativamente el aprendizaje.

En una versi√≥n determinista del entorno FrozenLake-v1, se podr√≠an requerir diferentes configuraciones de los par√°metros de Dyna-Q+. La bonificaci√≥n de exploraci√≥n podr√≠a ser menor, ya que la aleatoriedad del entorno ya no ser√≠a un factor, y se podr√≠an reducir los pasos de planificaci√≥n 'n' sin afectar tanto el rendimiento.


