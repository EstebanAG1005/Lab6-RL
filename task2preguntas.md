## Task 2
1. Estrategias de exploraci칩n:

a. 쮺칩mo influye la bonificaci칩n de exploraci칩n en Dyna-Q+ en la pol칤tica en comparaci칩n con el equilibrio de exploraci칩n-explotaci칩n en MCTS? 쯈u칠 enfoque conduce a una convergencia m치s r치pida en el entorno FrozenLake-v1?

La bonificaci칩n de exploraci칩n en Dyna-Q+ influye en la pol칤tica al promover una mayor exploraci칩n de estados desconocidos o menos visitados. Esto puede llevar a una convergencia m치s r치pida en el entorno FrozenLake-v1 en comparaci칩n con el equilibrio de exploraci칩n-explotaci칩n en MCTS. MCTS tiende a concentrarse m치s en la explotaci칩n de las acciones que han demostrado ser m치s prometedoras, lo que puede ralentizar la exploraci칩n de todo el espacio de estados. La bonificaci칩n de exploraci칩n en Dyna-Q+ ayuda a que el agente explore m치s activamente el entorno, lo que puede resultar en una convergencia m치s r치pida a la pol칤tica 칩ptima.

2. Rendimiento del algoritmo:

a. 쯈u칠 algoritmo, MCTS o Dyna-Q+, tuvo un mejor rendimiento en t칠rminos de tasa de 칠xito y recompensa promedio en el entorno FrozenLake-v1? Analice por qu칠 uno podr칤a superar al otro dada la naturaleza estoc치stica del entorno.

En general, el rendimiento de Dyna-Q+ suele ser mejor que MCTS en el entorno FrozenLake-v1, con una mayor tasa de 칠xito y recompensa promedio. Esto se debe a que Dyna-Q+ puede aprovechar mejor la naturaleza estoc치stica del entorno. Al actualizar el modelo interno y planificar utilizando este modelo, Dyna-Q+ puede anticipar y adaptarse mejor a las transiciones probabil칤sticas. Por otro lado, MCTS se basa m치s en la exploraci칩n aleatoria del 치rbol de b칰squeda, lo que puede ser menos eficiente en entornos con alta aleatoriedad.

3. Impacto de las transiciones estoc치sticas:

a. 쮺칩mo afectan las transiciones probabil칤sticas en FrozenLake-v1 al proceso de planificaci칩n en MCTS en comparaci칩n con Dyna-Q+? 쯈u칠 algoritmo es m치s robusto a la aleatoriedad introducid por el entorno?

Las transiciones probabil칤sticas en FrozenLake-v1 afectan m치s al proceso de planificaci칩n en MCTS que a Dyna-Q+. MCTS debe explorar el 치rbol de b칰squeda de forma m치s exhaustiva para tener en cuenta la aleatoriedad, lo que puede ralentizar la convergencia. En cambio, Dyna-Q+ puede aprovechar su modelo interno para anticipar y adaptarse mejor a las transiciones estoc치sticas. Esto hace que Dyna-Q+ sea m치s robusto a la aleatoriedad introducida por el entorno.

4. Sensibilidad de los par치metros:

a. En la implementaci칩n de Dyna-Q+, 쯖칩mo afecta el cambio de la cantidad de pasos de planificaci칩n 洧녵 y la bonificaci칩n de exploraci칩n a la curva de aprendizaje y al rendimiento final? 쯉e necesitar칤an diferentes configuraciones para una versi칩n determinista del entorno?

En la implementaci칩n de Dyna-Q+, el aumento del n칰mero de pasos de planificaci칩n 'n' y de la bonificaci칩n de exploraci칩n generalmente mejorar치 la curva de aprendizaje y el rendimiento final. Un mayor 'n' permite al agente planificar m치s a fondo utilizando su modelo interno, mientras que una mayor bonificaci칩n de exploraci칩n lo incentiva a explorar estados menos conocidos. Sin embargo, existe un equilibrio, ya que valores excesivamente altos de estos par치metros pueden llevar a una exploraci칩n demasiado agresiva y afectar negativamente el aprendizaje.

En una versi칩n determinista del entorno FrozenLake-v1, se podr칤an requerir diferentes configuraciones de los par치metros de Dyna-Q+. La bonificaci칩n de exploraci칩n podr칤a ser menor, ya que la aleatoriedad del entorno ya no ser칤a un factor, y se podr칤an reducir los pasos de planificaci칩n 'n' sin afectar tanto el rendimiento.